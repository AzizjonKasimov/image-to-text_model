{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3106\n",
      "Device being used: cuda\n",
      "Special tokens in vocabulary:\n",
      "<START>: 1\n",
      "<END>: 2\n",
      "<PAD>: 0\n",
      "<UNK>: 3\n",
      "\n",
      "Successfully generated caption: four be in of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from utils.vocabulary import build_vocabulary, load_vocabulary\n",
    "from models.model import ImageCaptioningModel\n",
    "import json\n",
    "\n",
    "class CaptionInference:\n",
    "    def __init__(self, model_path, vocabulary_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "\n",
    "        # Load and create vocabulary\n",
    "        self.vocab = load_vocabulary(vocabulary_path)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.embed_size = 256  # Make sure these match your training parameters\n",
    "        self.hidden_size = 512\n",
    "        self.num_layers = 2\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Create and load model\n",
    "        self.model = ImageCaptioningModel(\n",
    "            embed_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_layers=self.num_layers \n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load model weights\n",
    "        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def idx_to_word(self, idx):\n",
    "        for word, index in self.vocab.items():\n",
    "            if index == idx:\n",
    "                return word\n",
    "        return self.vocab['<UNK>']\n",
    "    \n",
    "    def generate_caption(self, image_path, max_length=50):\n",
    "        # Load and preprocess image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Generate caption\n",
    "        with torch.no_grad():\n",
    "            caption_indices = self.model.generate_caption(\n",
    "                image,\n",
    "                self.vocab,\n",
    "                max_length=max_length\n",
    "            )\n",
    "        \n",
    "        # Convert indices to words\n",
    "        caption_words = []\n",
    "        for idx in caption_indices:\n",
    "            word = self.idx_to_word(idx)\n",
    "            if word == '<END>':\n",
    "                break\n",
    "            if word not in ['<START>', '<PAD>', '<UNK>']:\n",
    "                caption_words.append(word)\n",
    "        \n",
    "        return ' '.join(caption_words)\n",
    "\n",
    "def test_inference():\n",
    "    # Initialize inference\n",
    "    inference = CaptionInference(\n",
    "        model_path=r'checkpoints\\model_epoch_10.pth',\n",
    "        vocabulary_path=r'data\\vocabulary'\n",
    "    )\n",
    "    \n",
    "    # Print some debug information\n",
    "    print(f\"Vocabulary size: {len(inference.vocab)}\")\n",
    "    print(f\"Device being used: {inference.device}\")\n",
    "    print(f\"Special tokens in vocabulary:\")\n",
    "    for token in ['<START>', '<END>', '<PAD>', '<UNK>']:\n",
    "        print(f\"{token}: {inference.vocab.get(token, 'Not found')}\")\n",
    "    \n",
    "    # Try generating a caption\n",
    "    try:\n",
    "        image_path = r'data\\Flicker8k_Dataset\\667626_18933d713e.jpg'\n",
    "        caption = inference.generate_caption(image_path)\n",
    "        print(f\"\\nSuccessfully generated caption: {caption}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during caption generation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgcap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
